{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# ---\n",
    "# title: \"matplotlib example\"\n",
    "# format:\n",
    "#   html:\n",
    "#     code-fold: true\n",
    "# ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "# Supress warnings.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.warn('DelftStack')\n",
    "warnings.warn('Do not show this message')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "# Importing libraries \n",
    "#| echo: false\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import janitor\n",
    "import sklearn \n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score, accuracy_score, recall_score, f1_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "data = pd.read_csv('../data/mushrooms.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of the dataset\n",
    "\n",
    "description, dictionary\n",
    "\n",
    "Mushroom anatomy picture\n",
    "\n",
    "Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Munging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Clean column names automatically by replacing each - with _ using  `pyjanitor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names before cleaning: \n",
      " ['class', 'cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor', 'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color', 'stalk-shape', 'stalk-root', 'stalk-surface-above-ring', 'stalk-surface-below-ring', 'stalk-color-above-ring', 'stalk-color-below-ring', 'veil-type', 'veil-color', 'ring-number', 'ring-type', 'spore-print-color', 'population', 'habitat'] \n",
      "\n",
      "\n",
      "Column names after cleaning: \n",
      " ['class', 'cap_shape', 'cap_surface', 'cap_color', 'bruises', 'odor', 'gill_attachment', 'gill_spacing', 'gill_size', 'gill_color', 'stalk_shape', 'stalk_root', 'stalk_surface_above_ring', 'stalk_surface_below_ring', 'stalk_color_above_ring', 'stalk_color_below_ring', 'veil_type', 'veil_color', 'ring_number', 'ring_type', 'spore_print_color', 'population', 'habitat']\n"
     ]
    }
   ],
   "source": [
    "#| echo: true\n",
    "print(\"Column names before cleaning:\", '\\n', data.columns.tolist(), '\\n\\n')\n",
    "data = data.clean_names()\n",
    "print(\"Column names after cleaning:\", '\\n', data.columns.tolist()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) See what different values each column contains\n",
    "\n",
    "From here, we can see that the `veil_type` has one single value and therefore is redundant and not informative so we can proceed with dropping it.\n",
    "All the mushrooms in our dataset have partial veils, so the column `veil_type` is not informative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class :   ['p' 'e']\n",
      "cap_shape :   ['x' 'b' 's' 'f' 'k' 'c']\n",
      "cap_surface :   ['s' 'y' 'f' 'g']\n",
      "cap_color :   ['n' 'y' 'w' 'g' 'e' 'p' 'b' 'u' 'c' 'r']\n",
      "bruises :   ['t' 'f']\n",
      "odor :   ['p' 'a' 'l' 'n' 'f' 'c' 'y' 's' 'm']\n",
      "gill_attachment :   ['f' 'a']\n",
      "gill_spacing :   ['c' 'w']\n",
      "gill_size :   ['n' 'b']\n",
      "gill_color :   ['k' 'n' 'g' 'p' 'w' 'h' 'u' 'e' 'b' 'r' 'y' 'o']\n",
      "stalk_shape :   ['e' 't']\n",
      "stalk_root :   ['e' 'c' 'b' 'r' '?']\n",
      "stalk_surface_above_ring :   ['s' 'f' 'k' 'y']\n",
      "stalk_surface_below_ring :   ['s' 'f' 'y' 'k']\n",
      "stalk_color_above_ring :   ['w' 'g' 'p' 'n' 'b' 'e' 'o' 'c' 'y']\n",
      "stalk_color_below_ring :   ['w' 'p' 'g' 'b' 'n' 'e' 'y' 'o' 'c']\n",
      "veil_type :   ['p']\n",
      "veil_color :   ['w' 'n' 'o' 'y']\n",
      "ring_number :   ['o' 't' 'n']\n",
      "ring_type :   ['p' 'e' 'l' 'f' 'n']\n",
      "spore_print_color :   ['k' 'n' 'u' 'h' 'w' 'r' 'o' 'y' 'b']\n",
      "population :   ['s' 'n' 'a' 'v' 'y' 'c']\n",
      "habitat :   ['u' 'g' 'm' 'd' 'p' 'w' 'l']\n"
     ]
    }
   ],
   "source": [
    "#| echo: false\n",
    "data.columns.tolist()\n",
    "for col in data.columns.tolist(): \n",
    "    print(col,':  ',data[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "data.drop('veil_type', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the column `stalk_root` has a non-alphanumeric value and it might need some munging.\n",
    "According to the dataset's documentation, the value '?' in `stalk_root` means that they are missing or unknown stalk root data. \n",
    "\n",
    "Let's see how many of these missing values we have to decide if it'd be okay to drop these rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "      <th>Fraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>3776</td>\n",
       "      <td>0.464796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>?</th>\n",
       "      <td>2480</td>\n",
       "      <td>0.305268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>1120</td>\n",
       "      <td>0.137863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>556</td>\n",
       "      <td>0.068439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r</th>\n",
       "      <td>192</td>\n",
       "      <td>0.023634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Count  Fraction\n",
       "b   3776  0.464796\n",
       "?   2480  0.305268\n",
       "e   1120  0.137863\n",
       "c    556  0.068439\n",
       "r    192  0.023634"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: true\n",
    "vals = data['stalk_root'].value_counts().index.values.tolist()\n",
    "\n",
    "NA_count = data['stalk_root'].value_counts().values\n",
    "\n",
    "NA_frac = data['stalk_root'].value_counts().to_list()\n",
    "NA_frac = [i/sum(NA_frac) for i in NA_frac]\n",
    "\n",
    "pd.DataFrame(zip(NA_count,NA_frac), columns=['Count','Fraction'], index= vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now we can see that if we drop the missing values in this column we're losing 30% of our data which accounts for about 2500 instances. \n",
    "Dropping the rows is not the best solution in this case. \n",
    "Therefore, we'll try to impute using KNN.\n",
    "Before that, the categorical value must be numerically encoded/labelled from 0 to n. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the order of values in this column before label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e', 'c', 'b', 'r', '?']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.stalk_root.unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the corresponding label to '?' values so we can impute them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 2, 1, 4, 0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: true\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "encoded_data = deepcopy(data)\n",
    "for i in encoded_data.columns.tolist():\n",
    "    encoded_data[i]= le.fit_transform(encoded_data[i])\n",
    "\n",
    "encoded_data['stalk_root'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corresponding label to '?' is 0. \n",
    "But, for the models to impute the missing data, we should replace each 0 with a NaN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stalk root column value counts before imputation: \n",
      " 1.0    3776\n",
      "3.0    1120\n",
      "2.0     556\n",
      "4.0     192\n",
      "Name: stalk_root, dtype: int64 \n",
      "\n",
      "Stalk root column value counts before imputation: \n",
      " 1    3776\n",
      "0    2480\n",
      "3    1120\n",
      "2     556\n",
      "4     192\n",
      "Name: stalk_root, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#| echo: true\n",
    "print(\"Stalk root column value counts before imputation: \\n\", encoded_data.replace({'stalk_root': {0: np.nan}}).stalk_root.value_counts(), '\\n')\n",
    "imputer = KNNImputer(missing_values = np.nan, n_neighbors=5, weights = 'distance')\n",
    "imputer.fit_transform(encoded_data[['stalk_root']])\n",
    "print(\"Stalk root column value counts before imputation: \\n\", encoded_data.stalk_root.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that KNNImputer didn't give us any useful results and we're again back on square 1. Therefore, we'll just drop the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data.drop('stalk_root', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see if the classes in our dataset are balanced or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "      <th>Fraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>4208</td>\n",
       "      <td>51.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p</th>\n",
       "      <td>3916</td>\n",
       "      <td>48.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Count  Fraction\n",
       "e   4208      51.8\n",
       "p   3916      48.2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "vals = data['class'].value_counts().index.values.tolist()\n",
    "\n",
    "class_count = data['class'].value_counts().values\n",
    "\n",
    "class_frac = data['class'].value_counts().to_list()\n",
    "class_frac = [round((i/sum(class_frac))*100, 2) for i in class_frac]\n",
    "\n",
    "pd.DataFrame(zip(class_count,class_frac), columns=['Count','Fraction'], index= vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are \"adequately\" balanced and there's no need for any oversampling techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature & Target Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only feature and target engineering step we need to perform is label encoding for our categorical variables and the categorical target. Let's identify which class is 1 and which class is 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class values before encoding:  ['p', 'e']\n",
      "Class values after encoding:  [1, 0]\n"
     ]
    }
   ],
   "source": [
    "#| echo: true\n",
    "print(\"Class values before encoding: \", data['class'].unique().tolist())\n",
    "print(\"Class values after encoding: \", encoded_data['class'].unique().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the `Poisonous (p)` class is now labelled `1`, while the `Edible (e)` class is `0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline models are the stepping stone on which AI developers base their initial assumptions of the direction they should take their developing. So, baseline models tend to be rule-based and understandable. Since we're aiming to perform binary classification, we chose to do `LogisticRegression` as a baseline model which we'll use comapre our refined models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second step after label encoding our data to prepare for modeling is to perform standard scaling (z-score).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "scaled_data = deepcopy(encoded_data)\n",
    "scaled_data.drop('class', axis=1, inplace = True)\n",
    "\n",
    "ss = StandardScaler()\n",
    "\n",
    "ss.fit(scaled_data)\n",
    "scaled_data = ss.transform(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third step is to split our data to training and testing sets\n",
    "\n",
    "75% of the data is used for training while the remaining 25% is for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = scaled_data\n",
    "y = encoded_data['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      " [[1011   43]\n",
      " [  62  915]]\n",
      "\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.96      0.95      1054\n",
      "           1       0.96      0.94      0.95       977\n",
      "\n",
      "    accuracy                           0.95      2031\n",
      "   macro avg       0.95      0.95      0.95      2031\n",
      "weighted avg       0.95      0.95      0.95      2031\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Baseline Logistic Regression Model\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Training \n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Predicting\n",
    "lr_pred = lr.predict(X_test)\n",
    "\n",
    "# Evaluating\n",
    "print('Confusion Matrix :\\n', confusion_matrix(y_test, lr_pred))\n",
    "print()\n",
    "print('Classification Report :\\n', classification_report(y_test, lr_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADD SOMETHING HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Filtering & Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset has 20 features which is considered high, therefore, means of feature filtering and dimensionality reduction is necessary. For this purpose, we've used 3 feature filtering/selection algorithms (`Variance Thresholding, Chi Square, and LASSO`), as well as `Principal Component Anlaysis` for dimensionality reduction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
