{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# ---\n",
    "# title: \"matplotlib example\"\n",
    "# format:\n",
    "#   html:\n",
    "#     code-fold: true\n",
    "# ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "# Supress warnings.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.warn('DelftStack')\n",
    "warnings.warn('Do not show this message')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "# Importing libraries \n",
    "#| echo: false\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import janitor\n",
    "import sklearn \n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score, accuracy_score, recall_score, f1_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from copy import deepcopy\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "data = pd.read_csv('../data/mushrooms.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of the dataset\n",
    "\n",
    "description, dictionary\n",
    "\n",
    "Mushroom anatomy picture\n",
    "\n",
    "Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Munging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Clean column names automatically by replacing each - with _ using  `pyjanitor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names before cleaning: \n",
      " ['class', 'cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor', 'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color', 'stalk-shape', 'stalk-root', 'stalk-surface-above-ring', 'stalk-surface-below-ring', 'stalk-color-above-ring', 'stalk-color-below-ring', 'veil-type', 'veil-color', 'ring-number', 'ring-type', 'spore-print-color', 'population', 'habitat'] \n",
      "\n",
      "\n",
      "Column names after cleaning: \n",
      " ['class', 'cap_shape', 'cap_surface', 'cap_color', 'bruises', 'odor', 'gill_attachment', 'gill_spacing', 'gill_size', 'gill_color', 'stalk_shape', 'stalk_root', 'stalk_surface_above_ring', 'stalk_surface_below_ring', 'stalk_color_above_ring', 'stalk_color_below_ring', 'veil_type', 'veil_color', 'ring_number', 'ring_type', 'spore_print_color', 'population', 'habitat']\n"
     ]
    }
   ],
   "source": [
    "#| echo: true\n",
    "print(\"Column names before cleaning:\", '\\n', data.columns.tolist(), '\\n\\n')\n",
    "data = data.clean_names()\n",
    "print(\"Column names after cleaning:\", '\\n', data.columns.tolist()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) See what different values each column contains\n",
    "\n",
    "From here, we can see that the `veil_type` has one single value and therefore is redundant and not informative so we can proceed with dropping it.\n",
    "All the mushrooms in our dataset have partial veils, so the column `veil_type` is not informative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class :   ['p' 'e']\n",
      "cap_shape :   ['x' 'b' 's' 'f' 'k' 'c']\n",
      "cap_surface :   ['s' 'y' 'f' 'g']\n",
      "cap_color :   ['n' 'y' 'w' 'g' 'e' 'p' 'b' 'u' 'c' 'r']\n",
      "bruises :   ['t' 'f']\n",
      "odor :   ['p' 'a' 'l' 'n' 'f' 'c' 'y' 's' 'm']\n",
      "gill_attachment :   ['f' 'a']\n",
      "gill_spacing :   ['c' 'w']\n",
      "gill_size :   ['n' 'b']\n",
      "gill_color :   ['k' 'n' 'g' 'p' 'w' 'h' 'u' 'e' 'b' 'r' 'y' 'o']\n",
      "stalk_shape :   ['e' 't']\n",
      "stalk_root :   ['e' 'c' 'b' 'r' '?']\n",
      "stalk_surface_above_ring :   ['s' 'f' 'k' 'y']\n",
      "stalk_surface_below_ring :   ['s' 'f' 'y' 'k']\n",
      "stalk_color_above_ring :   ['w' 'g' 'p' 'n' 'b' 'e' 'o' 'c' 'y']\n",
      "stalk_color_below_ring :   ['w' 'p' 'g' 'b' 'n' 'e' 'y' 'o' 'c']\n",
      "veil_type :   ['p']\n",
      "veil_color :   ['w' 'n' 'o' 'y']\n",
      "ring_number :   ['o' 't' 'n']\n",
      "ring_type :   ['p' 'e' 'l' 'f' 'n']\n",
      "spore_print_color :   ['k' 'n' 'u' 'h' 'w' 'r' 'o' 'y' 'b']\n",
      "population :   ['s' 'n' 'a' 'v' 'y' 'c']\n",
      "habitat :   ['u' 'g' 'm' 'd' 'p' 'w' 'l']\n"
     ]
    }
   ],
   "source": [
    "#| echo: false\n",
    "data.columns.tolist()\n",
    "for col in data.columns.tolist(): \n",
    "    print(col,':  ',data[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "data.drop('veil_type', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the column `stalk_root` has a non-alphanumeric value and it might need some munging.\n",
    "According to the dataset's documentation, the value '?' in `stalk_root` means that they are missing or unknown stalk root data. \n",
    "\n",
    "Let's see how many of these missing values we have to decide if it'd be okay to drop these rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "      <th>Fraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>3776</td>\n",
       "      <td>0.464796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>?</th>\n",
       "      <td>2480</td>\n",
       "      <td>0.305268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>1120</td>\n",
       "      <td>0.137863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>556</td>\n",
       "      <td>0.068439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r</th>\n",
       "      <td>192</td>\n",
       "      <td>0.023634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Count  Fraction\n",
       "b   3776  0.464796\n",
       "?   2480  0.305268\n",
       "e   1120  0.137863\n",
       "c    556  0.068439\n",
       "r    192  0.023634"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: true\n",
    "vals = data['stalk_root'].value_counts().index.values.tolist()\n",
    "\n",
    "NA_count = data['stalk_root'].value_counts().values\n",
    "\n",
    "NA_frac = data['stalk_root'].value_counts().to_list()\n",
    "NA_frac = [i/sum(NA_frac) for i in NA_frac]\n",
    "\n",
    "pd.DataFrame(zip(NA_count,NA_frac), columns=['Count','Fraction'], index= vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now we can see that if we drop the missing values in this column we're losing 30% of our data which accounts for about 2500 instances. \n",
    "Dropping the rows is not the best solution in this case. \n",
    "Therefore, we'll try to impute using KNN.\n",
    "Before that, the categorical value must be numerically encoded/labelled from 0 to n. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the order of values in this column before label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e', 'c', 'b', 'r', '?']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.stalk_root.unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the corresponding label to '?' values so we can impute them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 2, 1, 4, 0]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: true\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "encoded_data = deepcopy(data)\n",
    "for i in encoded_data.columns.tolist():\n",
    "    encoded_data[i]= le.fit_transform(encoded_data[i])\n",
    "\n",
    "encoded_data['stalk_root'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corresponding label to '?' is 0. \n",
    "But, for the models to impute the missing data, we should replace each 0 with a NaN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stalk root column value counts before imputation: \n",
      " 1.0    3776\n",
      "3.0    1120\n",
      "2.0     556\n",
      "4.0     192\n",
      "Name: stalk_root, dtype: int64 \n",
      "\n",
      "Stalk root column value counts before imputation: \n",
      " 1    3776\n",
      "0    2480\n",
      "3    1120\n",
      "2     556\n",
      "4     192\n",
      "Name: stalk_root, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#| echo: true\n",
    "print(\"Stalk root column value counts before imputation: \\n\", encoded_data.replace({'stalk_root': {0: np.nan}}).stalk_root.value_counts(), '\\n')\n",
    "imputer = KNNImputer(missing_values = np.nan, n_neighbors=5, weights = 'distance')\n",
    "imputer.fit_transform(encoded_data[['stalk_root']])\n",
    "print(\"Stalk root column value counts before imputation: \\n\", encoded_data.stalk_root.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that KNNImputer didn't give us any useful results and we're again back on square 1. Therefore, we'll just drop the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data.drop('stalk_root', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see if the classes in our dataset are balanced or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "      <th>Fraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>4208</td>\n",
       "      <td>51.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p</th>\n",
       "      <td>3916</td>\n",
       "      <td>48.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Count  Fraction\n",
       "e   4208      51.8\n",
       "p   3916      48.2"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "vals = data['class'].value_counts().index.values.tolist()\n",
    "\n",
    "class_count = data['class'].value_counts().values\n",
    "\n",
    "class_frac = data['class'].value_counts().to_list()\n",
    "class_frac = [round((i/sum(class_frac))*100, 2) for i in class_frac]\n",
    "\n",
    "pd.DataFrame(zip(class_count,class_frac), columns=['Count','Fraction'], index= vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are \"adequately\" balanced and there's no need for any oversampling techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature & Target Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only feature and target engineering step we need to perform is label encoding for our categorical variables and the categorical target. Let's identify which class is 1 and which class is 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class values before encoding:  ['p', 'e']\n",
      "Class values after encoding:  [1, 0]\n"
     ]
    }
   ],
   "source": [
    "#| echo: true\n",
    "print(\"Class values before encoding: \", data['class'].unique().tolist())\n",
    "print(\"Class values after encoding: \", encoded_data['class'].unique().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the `Poisonous (p)` class is now labelled `1`, while the `Edible (e)` class is `0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline models are the stepping stone on which AI developers base their initial assumptions of the direction they should take their developing. So, baseline models tend to be rule-based and understandable. Since we're aiming to perform binary classification, we chose to do `LogisticRegression` as a baseline model which we'll use comapre our refined models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second step after label encoding our data to prepare for modeling is to perform standard scaling (z-score).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "scaled_data = deepcopy(encoded_data)\n",
    "scaled_data.drop('class', axis=1, inplace = True)\n",
    "\n",
    "ss = StandardScaler()\n",
    "\n",
    "ss.fit(scaled_data)\n",
    "scaled_data = ss.transform(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third step is to split our data to training and testing sets\n",
    "\n",
    "75% of the data is used for training while the remaining 25% is for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = scaled_data\n",
    "y = encoded_data['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      " [[965  43]\n",
      " [ 56 967]]\n",
      "\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.95      1008\n",
      "           1       0.96      0.95      0.95      1023\n",
      "\n",
      "    accuracy                           0.95      2031\n",
      "   macro avg       0.95      0.95      0.95      2031\n",
      "weighted avg       0.95      0.95      0.95      2031\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Baseline Logistic Regression Model\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Training \n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Predicting\n",
    "lr_pred = lr.predict(X_test)\n",
    "\n",
    "# Evaluating\n",
    "print('Confusion Matrix :\\n', confusion_matrix(y_test, lr_pred))\n",
    "print()\n",
    "print('Classification Report :\\n', classification_report(y_test, lr_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Filtering & Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset has 20 features which is considered high, therefore, means of feature filtering and dimensionality reduction is necessary. For this purpose, we've used 3 feature filtering/selection algorithms (`Variance Thresholding, Chi Square, and LASSO`), as well as `Principal Component Anlaysis` for dimensionality reduction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Thresholding\n",
    "The first feature filtering method we used was variance thresholding. Features with low variance are considered redundant and non-informative. As a rule of thumb, features that have 5-10% > variance are removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['cap_shape', 'cap_surface', 'cap_color', 'bruises', 'odor',\n",
       "       'gill_spacing', 'gill_size', 'gill_color', 'stalk_shape',\n",
       "       'stalk_surface_above_ring', 'stalk_surface_below_ring',\n",
       "       'stalk_color_above_ring', 'stalk_color_below_ring', 'ring_type',\n",
       "       'spore_print_color', 'population', 'habitat'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def variance_threshold_selector(data, threshold=0.1):\n",
    "    selector = VarianceThreshold(threshold)\n",
    "    selector.fit(data)\n",
    "    return data.columns[selector.get_support(indices=True)]\n",
    "\n",
    "variance_threshold_cols = variance_threshold_selector(encoded_data.drop('class', axis = 1))\n",
    "variance_threshold_cols # Threshold values 5% and 10% are a rule of thumb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that 10% variance resulted in dropping 2 column only (`ring_number` and `veil_color`), while 5% didn't do us any good. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the effect of removing these two features on the accuracy using our baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, the recall and f1-score of the positive class decreased which means that the beaseline model deals better with classifying the positive class than this model. So, this method is not beneficial to us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Absolute Shrinkage and Selection Operator (LASSO)\n",
    "The second method is LASSO `L1` which is a regularizaion method that banishes the weights of redundant features to reduce overfitting but they can be used for feature selection. The strength of regularization is determined by the variable `C` which is the strength of the penalty term. As `C` decreases the strength increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total features: 20\n",
      "selected features: 12\n",
      "features with coefficients shrank to zero: 8\n",
      "\n",
      "Selected columns using Lasso: \n",
      " ['cap_shape', 'cap_color', 'bruises', 'gill_attachment', 'gill_spacing', 'gill_size', 'stalk_shape', 'stalk_root', 'stalk_surface_above_ring', 'stalk_surface_below_ring', 'stalk_color_above_ring', 'ring_type'] \n",
      "\n",
      "Confusion Matrix :\n",
      " [[1006   45]\n",
      " [  62  918]]\n",
      "\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.96      0.95      1051\n",
      "           1       0.95      0.94      0.94       980\n",
      "\n",
      "    accuracy                           0.95      2031\n",
      "   macro avg       0.95      0.95      0.95      2031\n",
      "weighted avg       0.95      0.95      0.95      2031\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lasso \n",
    "# Start by splitting\n",
    "X = scaled_data\n",
    "y = encoded_data['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "# Select features from the model\n",
    "sel_ = SelectFromModel(LogisticRegression(C=0.01, penalty='l1', solver='liblinear'))\n",
    "sel_.fit(X_train, np.ravel(y_train,order='C'))\n",
    "sel_.get_support()\n",
    "X_train = pd.DataFrame(X_train)\n",
    "\n",
    "\n",
    "# To view the set of selected features\n",
    "selected_feat = X_train.columns[(sel_.get_support())]\n",
    "print('total features: {}'.format((X_train.shape[1])))\n",
    "print('selected features: {}'.format(len(selected_feat)))\n",
    "print('features with coefficients shrank to zero: {}'.format(\n",
    "np.sum(sel_.estimator_.coef_ == 0)))\n",
    "print()\n",
    "\n",
    "sel_col_l1 = list()\n",
    "[sel_col_l1.append(data.columns[i]) for i in X_train.columns[sel_.get_support(indices=True)].tolist()]\n",
    "print('Selected columns using Lasso:', '\\n',sel_col_l1, '\\n')\n",
    "\n",
    "# Logistic Regression\n",
    "lr = LogisticRegression(max_iter=500)\n",
    "\n",
    "# training & prediction\n",
    "lr.fit(X_train, y_train)\n",
    "lr_pred = lr.predict(X_test)\n",
    "\n",
    "# evaluation\n",
    "print('Confusion Matrix :\\n', confusion_matrix(y_test, lr_pred))\n",
    "print()\n",
    "print('Classification Report :\\n', classification_report(y_test, lr_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LASSO with `C = 0.01` resulted in removing 8 features `{cap_surface, odor, gill_color, stalk_color_below_ring, ring_number, habitat, population, veil_color, spore_print_color}` only to get a similar performance to the beasline model. However, this model is favored since it can get the same performance with 8 less features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi Squared\n",
    "Some shit about it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 0: 13.923331\n",
      "Feature 1: 156.656338\n",
      "Feature 2: 10.472532\n",
      "Feature 3: 886.750744\n",
      "Feature 4: 44.583778\n",
      "Feature 5: 3.023020\n",
      "Feature 6: 611.221041\n",
      "Feature 7: 1230.183885\n",
      "Feature 8: 4419.991956\n",
      "Feature 9: 28.005064\n",
      "Feature 10: 169.817457\n",
      "Feature 11: 158.063962\n",
      "Feature 12: 98.263204\n",
      "Feature 13: 85.534474\n",
      "Feature 14: 4.372503\n",
      "Feature 15: 17.051667\n",
      "Feature 16: 1485.578593\n",
      "Feature 17: 282.895674\n",
      "Feature 18: 230.428256\n",
      "Feature 19: 603.775531\n"
     ]
    }
   ],
   "source": [
    "def select_features(X_train, y_train, X_test):\n",
    "    fs = SelectKBest(score_func=chi2, k='all')\n",
    "    fs.fit(X_train, y_train)\n",
    "    X_train_fs = fs.transform(X_train)\n",
    "    X_test_fs = fs.transform(X_test)\n",
    "    return X_train_fs, X_test_fs, fs\n",
    "X = encoded_data.drop('class', axis = 1)\n",
    "y = encoded_data['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "X_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test)\n",
    "\n",
    "for i in range(len(fs.scores_)):\n",
    "    print('Feature %d: %f' % (i, fs.scores_[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Here we will select features with higher importance score. So by looking at them we may select feature (1, 3, 6, 7, 8, 10, 11, 12, 17, 18, 19 and 20). \n",
    "Basically I have selected feature with importance score of more than 100,\n",
    "considering there is no rule that you have to do selection in this way only,\n",
    "it is completely on you what importance score you chose as a deciding factor, but one has to take the features with higher score.\n",
    "#They are:-\n",
    "\n",
    "#1 - \"cap-shape\"\n",
    "\n",
    "#3 - \"bruises\"\n",
    "\n",
    "#6 - \"gill-spacing\"\n",
    "\n",
    "#7 - \"gill-size\"\n",
    "\n",
    "#8 - \"gill-color\"\n",
    "\n",
    "#10 - \"stalk-root\"\n",
    "\n",
    "#11 - \"stalk-surface-above-ring\n",
    "\n",
    "#12 - \"stalk-surface-below-ring\"\n",
    "\n",
    "#17 - \"ring-type\"\n",
    "\n",
    "#18 - \"spore-print-color\"\n",
    "\n",
    "#19 - \"population\"\n",
    "\n",
    "#20 - \"habitat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      " [[1006   43]\n",
      " [  96  886]]\n",
      "\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.96      0.94      1049\n",
      "           1       0.95      0.90      0.93       982\n",
      "\n",
      "    accuracy                           0.93      2031\n",
      "   macro avg       0.93      0.93      0.93      2031\n",
      "weighted avg       0.93      0.93      0.93      2031\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = encoded_data[['cap_shape', 'bruises', 'gill_spacing', 'gill_size', 'gill_color', 'stalk_surface_above_ring', 'stalk_surface_below_ring','ring_type', 'spore_print_color','population','habitat']]\n",
    "y = encoded_data['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "# Logistic Regression\n",
    "\n",
    "\n",
    "lr = LogisticRegression(max_iter=500)\n",
    "\n",
    "# training & prediction\n",
    "lr.fit(ss.fit_transform(X_train), y_train)\n",
    "lr_pred = lr.predict(ss.fit_transform(X_test))\n",
    "\n",
    "# evaluation\n",
    "print('Confusion Matrix :\\n', confusion_matrix(y_test, lr_pred))\n",
    "print()\n",
    "print('Classification Report :\\n', classification_report(y_test, lr_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is better with the negative class which is opposite to the variance thresholding. We'll discard it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
